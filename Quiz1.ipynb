{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quiz2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOw0YV8AWT85PpFVDLuhyy/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BigPapa958/AIML/blob/main/Quiz1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gF_CyjuQDXoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from matplotlib import pyplot\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score, mean_absolute_error\n",
        "from collections import Counter\n",
        "\n",
        "import keras # importing keras library\n",
        "from keras.models import Sequential  # importing the Sequential Model\n",
        "from keras.layers import Dense       # importing Dense layer\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import Dense, Dropout,InputLayer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "0kPodraGDfQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af1b6b80-112b-476e-b948-06a8ae4daa32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_srhSS-iFjlj",
        "outputId": "0696a845-4c08-4b6e-9135-552c02c92d16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Onk2M0IrFolI",
        "outputId": "927526e2-52ae-4524-8051-bcb49ea5222f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))"
      ],
      "metadata": {
        "id": "3cZ8EqfGHHhi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the image pixels\n",
        "X_train_normalized = X_train.astype('float32')/255.0\n",
        "X_test_normalized = X_test.astype('float32')/255.0"
      ],
      "metadata": {
        "id": "P8lAYIi8HIdi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating one-hot encoded representation of target labels\n",
        "# We can do this by using this utility function - https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n",
        "# to_categorical() function is also explained in the Neural Networks Module\n",
        "\n",
        "y_train_encoded = tf.keras.utils.to_categorical(y_train)\n",
        "y_test_encoded = tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "10TqyO_PHNIv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld8Vd-CMHSEx",
        "outputId": "e0871b33-e77f-40d1-a286-5514894ca38c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "np.random.seed(1) #for numpy\n",
        "random.seed(1) \n",
        "tf.random.set_seed(1) #for tensorflow"
      ],
      "metadata": {
        "id": "yxElvWuaDnJC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w9fUC5unHgqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intializing a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Adding first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n",
        "# Input_shape denotes input image dimension of MNIST images\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1), padding = 'same')) #padding = 'same'\n",
        "\n",
        "# Adding max pooling to reduce the size of output of first conv layer\n",
        "model.add(MaxPooling2D((2, 2), padding = 'same'))\n",
        "\n",
        "# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n",
        "model.add(Flatten())\n",
        "\n",
        "# Adding a fully connected dense layer with 100 neurons    \n",
        "model.add(Dense(100, activation='relu'))\n",
        "\n",
        "# Adding the output layer with 10 neurons and activation functions as softmax since this is a multi-class classification problem  \n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Using SGD Optimizer\n",
        "opt = Adam()\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) #, metrics=['accuracy']\n",
        "\n",
        "# Generating the summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-Zf7rXRFi2l",
        "outputId": "18e4b745-d35b-4804-9deb-046d29c8711b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12544)             0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 100)               1254500   \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,256,150\n",
            "Trainable params: 1,256,150\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_1 = model.fit(\n",
        "            X_train_normalized, y_train_encoded,\n",
        "            epochs=5,\n",
        "            #validation_split=0.1,\n",
        "            #shuffle=True,\n",
        "            #batch_size=64,\n",
        "            verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlNQKfIVJAO5",
        "outputId": "6703aa84-c876-4e2b-9f39-c3212dfa5710"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 - 8s - loss: 0.1368 - accuracy: 0.9593 - 8s/epoch - 5ms/step\n",
            "Epoch 2/5\n",
            "1875/1875 - 8s - loss: 0.0477 - accuracy: 0.9854 - 8s/epoch - 4ms/step\n",
            "Epoch 3/5\n",
            "1875/1875 - 8s - loss: 0.0304 - accuracy: 0.9908 - 8s/epoch - 4ms/step\n",
            "Epoch 4/5\n",
            "1875/1875 - 8s - loss: 0.0197 - accuracy: 0.9937 - 8s/epoch - 4ms/step\n",
            "Epoch 5/5\n",
            "1875/1875 - 8s - loss: 0.0124 - accuracy: 0.9960 - 8s/epoch - 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = model.evaluate(X_test_normalized, y_test_encoded, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM5YIFfELTiC",
        "outputId": "5c02d54f-fba2-49bc-cc00-b76932e00771"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0487 - accuracy: 0.9864 - 1s/epoch - 4ms/step\n"
          ]
        }
      ]
    }
  ]
}